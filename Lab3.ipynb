{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ccc4da6-e60d-48b8-bb44-3bec19826382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7df5d71-596e-46f2-a865-2e912cfd0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://dataquestio.github.io/web-scraping-pages/simple.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35798ae8-1151-4e8b-baab-3dea08c7ba29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a76f31e4-a7e6-44dd-b680-d418fc96437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87360c4-2bd2-41eb-808d-14014a481e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['html',\n",
       " '\\n',\n",
       " <html>\n",
       " <head>\n",
       " <title>A simple example page</title>\n",
       " </head>\n",
       " <body>\n",
       " <p>Here is some simple content for this page.</p>\n",
       " </body>\n",
       " </html>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(soup.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c5e9af-6a71-46b0-a66d-6b4aa840de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = list(soup.children)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a416004-c9c2-4d04-b5dc-024836ca936c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>A simple example page</title>\n",
       "</head>\n",
       "<body>\n",
       "<p>Here is some simple content for this page.</p>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a301752-afbe-4e85-969e-dad5bd9d8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Content: Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "content = soup.find('p').get_text()\n",
    "print(\"Extracted Content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e090271-2764-42b8-b9b2-44ec97ca04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_data1.txt\", \"w\") as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390c10f-545c-49d9-887b-d45b41b15f05",
   "metadata": {},
   "source": [
    "## Code to scrape specific content from single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcd4c8f7-105b-4119-adbf-6500cd463a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote:  “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "Author:  Albert Einstein\n",
      "Tags:  change, deep-thoughts, thinking, world\n",
      "\n",
      "Quote:  “It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "Author:  J.K. Rowling\n",
      "Tags:  abilities, choices\n",
      "\n",
      "Quote:  “There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "Author:  Albert Einstein\n",
      "Tags:  inspirational, life, live, miracle, miracles\n",
      "\n",
      "Quote:  “The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "Author:  Jane Austen\n",
      "Tags:  aliteracy, books, classic, humor\n",
      "\n",
      "Quote:  “Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "Author:  Marilyn Monroe\n",
      "Tags:  be-yourself, inspirational\n",
      "\n",
      "Quote:  “Try not to become a man of success. Rather become a man of value.”\n",
      "Author:  Albert Einstein\n",
      "Tags:  adulthood, success, value\n",
      "\n",
      "Quote:  “It is better to be hated for what you are than to be loved for what you are not.”\n",
      "Author:  André Gide\n",
      "Tags:  life, love\n",
      "\n",
      "Quote:  “I have not failed. I've just found 10,000 ways that won't work.”\n",
      "Author:  Thomas A. Edison\n",
      "Tags:  edison, failure, inspirational, paraphrased\n",
      "\n",
      "Quote:  “A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "Author:  Eleanor Roosevelt\n",
      "Tags:  misattributed-eleanor-roosevelt\n",
      "\n",
      "Quote:  “A day without sunshine is like, you know, night.”\n",
      "Author:  Steve Martin\n",
      "Tags:  humor, obvious, simile\n",
      "\n",
      "Quotes have been saved to quotes.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Send a GET request to the website\n",
    "# Similar to your first image: page = requests.get(...)\n",
    "page = requests.get('https://quotes.toscrape.com')\n",
    "\n",
    "# Parse the HTML content\n",
    "# Similar to your second image: soup = BeautifulSoup(...)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# Create a list to store quotes\n",
    "quotes = []\n",
    "\n",
    "# Find all quote elements\n",
    "quote_elements = soup.find_all('div', class_='quote')\n",
    "\n",
    "# Extract information from each quote element\n",
    "for quote_element in quote_elements:\n",
    "    # extract the text of the quote\n",
    "    # Similar to your third image: soup.find('p').get_text()\n",
    "    text = quote_element.find('span', class_='text').text\n",
    "    \n",
    "    # extract the author of the quote\n",
    "    author = quote_element.find('small', class_='author').text\n",
    "    \n",
    "    # extract the tag <a> HTML elements related to the quote\n",
    "    tag_elements = quote_element.select('.tags .tag')\n",
    "    \n",
    "    # store the list of tag strings in a list\n",
    "    tags = []\n",
    "    for tag_element in tag_elements:\n",
    "        tags.append(tag_element.text)\n",
    "\n",
    "    # FIXED: This block must be indented inside the 'for quote_element' loop\n",
    "    quotes.append(\n",
    "        {\n",
    "            'text': text,\n",
    "            'author': author,\n",
    "            'tags': ', '.join(tags) # merge the tags into a \"A, B, ..., Z\" string\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Print the scraped quotes - optional\n",
    "for quote in quotes:\n",
    "    print(\"Quote: \", quote['text'])\n",
    "    print(\"Author: \", quote['author'])\n",
    "    # FIXED: This print was outside the loop in your original code\n",
    "    print(\"Tags: \", quote['tags'])\n",
    "    print()\n",
    "\n",
    "# Save quotes to a CSV file\n",
    "# Similar to your fourth image: with open(...) as file:\n",
    "with open('quotes.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    fieldnames = ['text', 'author', 'tags']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write headers\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write quotes\n",
    "    # FIXED: Indentation fixed for the writing loop\n",
    "    for quote in quotes:\n",
    "        writer.writerow(quote)\n",
    "\n",
    "print(\"Quotes have been saved to quotes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbc780-3279-4aa1-81a0-0dd37bc2275f",
   "metadata": {},
   "source": [
    "## Print Specific Content of Multiple Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7071941-0f4d-41df-b3fe-771c7139b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://quotes.toscrape.com\n",
      "Scraping: https://quotes.toscrape.com/page/2/\n",
      "Scraping: https://quotes.toscrape.com/page/3/\n",
      "Scraping: https://quotes.toscrape.com/page/4/\n",
      "Scraping: https://quotes.toscrape.com/page/5/\n",
      "Scraping: https://quotes.toscrape.com/page/6/\n",
      "Scraping: https://quotes.toscrape.com/page/7/\n",
      "Scraping: https://quotes.toscrape.com/page/8/\n",
      "Scraping: https://quotes.toscrape.com/page/9/\n",
      "Scraping: https://quotes.toscrape.com/page/10/\n",
      "Finished! Saved 100 quotes to quotes2.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Function to scrape quotes from a page\n",
    "def scrape_page(soup, quotes):\n",
    "    # Fixed indentation: all code inside the function must be indented\n",
    "    for quote in soup.find_all('div', class_='quote'):\n",
    "        text = quote.find('span', class_='text').text\n",
    "        author = quote.find('small', class_='author').text\n",
    "        tags = ', '.join(tag.text for tag in quote.find_all('a', class_='tag'))\n",
    "        quotes.append({'Text': text, 'Author': author, 'Tags': tags})\n",
    "\n",
    "# Base URL and headers\n",
    "base_url = 'https://quotes.toscrape.com'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# List to store quotes\n",
    "quotes = []\n",
    "\n",
    "# Function to scrape quotes from multiple pages\n",
    "def scrape_all_pages(url):\n",
    "    while url:\n",
    "        print(f\"Scraping: {url}\") # Progress tracker\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            scrape_page(soup, quotes)\n",
    "            \n",
    "            # Look for the 'Next' button\n",
    "            next_page = soup.find('li', class_='next')\n",
    "            # Fixed indentation: URL update must be inside the while loop\n",
    "            url = base_url + next_page.find('a')['href'] if next_page else None\n",
    "        else:\n",
    "            print(\"Failed to retrieve the page.\")\n",
    "            break\n",
    "\n",
    "# Scrape quotes from all pages\n",
    "scrape_all_pages(base_url)\n",
    "\n",
    "# Save quotes to CSV file\n",
    "with open('quotes2.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['Text', 'Author', 'Tags'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(quotes)\n",
    "\n",
    "print(f\"Finished! Saved {len(quotes)} quotes to quotes2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075bc0d5-a16e-4f98-a087-0cc12b866afc",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa6ae4-8372-4b6b-996a-442cf6465cde",
   "metadata": {},
   "source": [
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9ef873d-207c-40b1-b825-b10d0175e7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking page: https://quotes.toscrape.com\n",
      "Checking page: https://quotes.toscrape.com/page/2/\n",
      "Checking page: https://quotes.toscrape.com/page/3/\n",
      "Checking page: https://quotes.toscrape.com/page/4/\n",
      "Checking page: https://quotes.toscrape.com/page/5/\n",
      "Checking page: https://quotes.toscrape.com/page/6/\n",
      "Checking page: https://quotes.toscrape.com/page/7/\n",
      "Checking page: https://quotes.toscrape.com/page/8/\n",
      "Checking page: https://quotes.toscrape.com/page/9/\n",
      "Checking page: https://quotes.toscrape.com/page/10/\n",
      "\n",
      "Success! Found 10 quotes by Albert Einstein.\n",
      "Data saved to 'quotes_Einstein.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Function to scrape quotes from a page and filter by author\n",
    "def scrape_page(soup, quotes, target_author):\n",
    "    for quote in soup.find_all('div', class_='quote'):\n",
    "        author = quote.find('small', class_='author').text\n",
    "        \n",
    "        # Only extract data if the author matches our target\n",
    "        if author == target_author:\n",
    "            text = quote.find('span', class_='text').text\n",
    "            tags = ', '.join(tag.text for tag in quote.find_all('a', class_='tag'))\n",
    "            quotes.append({'Text': text, 'Author': author, 'Tags': tags})\n",
    "\n",
    "# Configuration\n",
    "base_url = 'https://quotes.toscrape.com'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "target_author = \"Albert Einstein\"\n",
    "all_quotes = []\n",
    "\n",
    "def scrape_all_pages(url):\n",
    "    while url:\n",
    "        print(f\"Checking page: {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            scrape_page(soup, all_quotes, target_author)\n",
    "            \n",
    "            # Find the 'Next' button to continue to the next page\n",
    "            next_page = soup.find('li', class_='next')\n",
    "            url = base_url + next_page.find('a')['href'] if next_page else None\n",
    "            \n",
    "            # Brief pause to be polite to the server\n",
    "            time.sleep(0.5) \n",
    "        else:\n",
    "            print(\"Error accessing the page.\")\n",
    "            break\n",
    "\n",
    "# Run the scraper\n",
    "scrape_all_pages(base_url)\n",
    "\n",
    "# Save the specific Einstein quotes to a CSV\n",
    "with open('quotes_Einstein.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['Text', 'Author', 'Tags'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_quotes)\n",
    "\n",
    "print(f\"\\nSuccess! Found {len(all_quotes)} quotes by {target_author}.\")\n",
    "print(\"Data saved to 'quotes_Einstein.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a238f72-0545-44a5-92b2-ee648d5ae6ef",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b165e65-0bda-421c-ab9a-6fb9f01c242f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: “There are only two ways to live your life. One is...\n",
      "Found: “This life is what you make it. No matter what, yo...\n",
      "Found: “The opposite of love is not hate, it's indifferen...\n",
      "Found: “Good friends, good books, and a sleepy conscience...\n",
      "Found: “Life is what happens to us while we are making ot...\n",
      "Found: “Life is like riding a bicycle. To keep your balan...\n",
      "Found: “If I were not a physicist, I would probably be a ...\n",
      "Found: “Life isn't about finding yourself. Life is about ...\n",
      "Found: “The fear of death follows from the fear of life. ...\n",
      "Found: “I'm the one that's got to die when it's time for ...\n",
      "\n",
      "Done! Saved 10 quotes containing 'life' to 'life_quotes.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Function to scrape quotes containing a specific keyword\n",
    "def scrape_page(soup, quotes, keyword):\n",
    "    for quote in soup.find_all('div', class_='quote'):\n",
    "        text = quote.find('span', class_='text').text\n",
    "        \n",
    "        # Check if the keyword exists in the quote text (case-insensitive)\n",
    "        if keyword.lower() in text.lower():\n",
    "            author = quote.find('small', class_='author').text\n",
    "            tags = ', '.join(tag.text for tag in quote.find_all('a', class_='tag'))\n",
    "            \n",
    "            quotes.append({\n",
    "                'Text': text, \n",
    "                'Author': author, \n",
    "                'Tags': tags\n",
    "            })\n",
    "            # Optional: Print to console as requested\n",
    "            print(f\"Found: {text[:50]}...\")\n",
    "\n",
    "# Configuration\n",
    "base_url = 'https://quotes.toscrape.com'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "search_word = \"life\"\n",
    "life_quotes = []\n",
    "\n",
    "def scrape_all_pages(url):\n",
    "    while url:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            scrape_page(soup, life_quotes, search_word)\n",
    "            \n",
    "            # Pagination logic\n",
    "            next_page = soup.find('li', class_='next')\n",
    "            url = base_url + next_page.find('a')['href'] if next_page else None\n",
    "            time.sleep(0.5) # Polite scraping delay\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Execute the scraper\n",
    "scrape_all_pages(base_url)\n",
    "\n",
    "# Save the filtered quotes to a CSV\n",
    "with open('life_quotes.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['Text', 'Author', 'Tags'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(life_quotes)\n",
    "\n",
    "print(f\"\\nDone! Saved {len(life_quotes)} quotes containing '{search_word}' to 'life_quotes.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5be68-e1ff-49cb-81d7-d7850c36cf84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
