{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1ba0b9-9ddf-480f-b96b-96cc64938348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\im11\\anaconda3\\lib\\site-packages (2.15.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9df45c-ee0c-4de1-bdb3-65ddf6623b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\im11\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7183fe4-2e90-48bc-98de-302081bd09ce",
   "metadata": {},
   "source": [
    "## Step 1: Read the source data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45da22b0-0977-4d52-95b5-d3aa28c1537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           Review\n",
      "0   The product arrived on time. Packaging was great, and the quality is amazing!\n",
      "1                                        THIS PRODUCT IS JUST AMAZING! I LOVE IT.\n",
      "2     I bought this phone for $799, and it has a 120Hz display. Totally worth it!\n",
      "3                         Wow!!! This product is awesome... but a bit expensive??\n",
      "4                                             The laptop works perfectly fine.   \n",
      "5    Check out the full product details here: https://example.com/product-details\n",
      "6         <div><h2>Great Purchase!</h2><p>I am happy with this product.</p></div>\n",
      "7                The battry life is excelent, but the chargin cable is too short.\n",
      "8                       I can't believe it's so good! Didn't expect such quality.\n",
      "9                   Love this product! ???? Fast delivery ??, amazing quality! ??\n",
      "10                       TBH, I wasnt expecting much, but OMG, this is awesome!!\n",
      "11                          This is the best product I have ever used in my life!\n",
      "12  The shoes were comfortable, fitting nicely, and worked perfectly for jogging.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "file_path = \"Review.csv\"\n",
    "df = pd.read_csv(file_path, encoding='latin1')\n",
    "# Display column content without truncation\n",
    "pd.set_option('display.max_colwidth', None) # Set to None for unlimited width\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f81c7-1ab3-49e6-aee5-069f90ec07ab",
   "metadata": {},
   "source": [
    "## Step 2: Perform Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67462dca-2eab-4182-8d70-391034129105",
   "metadata": {},
   "source": [
    "### a. Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b3da82-0b55-4707-a83d-a89de5bd84ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the product arrived on time. packaging was great, and the quality is amazing!\n",
      "1                                          this product is just amazing! i love it.\n",
      "2       i bought this phone for $799, and it has a 120hz display. totally worth it!\n",
      "3                           wow!!! this product is awesome... but a bit expensive??\n",
      "4                                               the laptop works perfectly fine.   \n",
      "5      check out the full product details here: https://example.com/product-details\n",
      "6           <div><h2>great purchase!</h2><p>i am happy with this product.</p></div>\n",
      "7                  the battry life is excelent, but the chargin cable is too short.\n",
      "8                         i can't believe it's so good! didn't expect such quality.\n",
      "9                     love this product! ???? fast delivery ??, amazing quality! ??\n",
      "10                         tbh, i wasnt expecting much, but omg, this is awesome!!\n",
      "11                            this is the best product i have ever used in my life!\n",
      "12    the shoes were comfortable, fitting nicely, and worked perfectly for jogging.\n",
      "Name: lowercased, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lowercase conversion\n",
    "def convert_to_lowercase(text):\n",
    " return text.lower()\n",
    "df[\"lowercased\"] = df[\"Review\"].apply(convert_to_lowercase)\n",
    "# Display column content without truncation\n",
    "pd.set_option('display.max_colwidth', None) # Set to None for unlimited width\n",
    "print(df[\"lowercased\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3214c630-b288-4767-86b2-1c262a197cf0",
   "metadata": {},
   "source": [
    "### b. Remove URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "214ef80b-93b5-43f5-92c0-fd023afac1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the product arrived on time. packaging was great, and the quality is amazing!\n",
      "1                                          this product is just amazing! i love it.\n",
      "2       i bought this phone for $799, and it has a 120hz display. totally worth it!\n",
      "3                           wow!!! this product is awesome... but a bit expensive??\n",
      "4                                               the laptop works perfectly fine.   \n",
      "5                                         check out the full product details here: \n",
      "6           <div><h2>great purchase!</h2><p>i am happy with this product.</p></div>\n",
      "7                  the battry life is excelent, but the chargin cable is too short.\n",
      "8                         i can't believe it's so good! didn't expect such quality.\n",
      "9                     love this product! ???? fast delivery ??, amazing quality! ??\n",
      "10                         tbh, i wasnt expecting much, but omg, this is awesome!!\n",
      "11                            this is the best product i have ever used in my life!\n",
      "12    the shoes were comfortable, fitting nicely, and worked perfectly for jogging.\n",
      "Name: urls_removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removal of URLs\n",
    "import re\n",
    "# remove any URLs that start with \"http\" or \"www\" from the text\n",
    "def remove_urls(text):\n",
    " return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "df[\"urls_removed\"] = df[\"lowercased\"].apply(remove_urls)\n",
    "# Display column content without truncation\n",
    "pd.set_option('display.max_colwidth', None) # Set to None for unlimited width\n",
    "print(df[\"urls_removed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972a785-960c-44b8-b511-ef7ae3a7bf23",
   "metadata": {},
   "source": [
    "### c. Remove HTML tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd05f27-be27-4d48-b65c-c2f7ef6591ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the product arrived on time. packaging was great, and the quality is amazing!\n",
      "1                                          this product is just amazing! i love it.\n",
      "2       i bought this phone for $799, and it has a 120hz display. totally worth it!\n",
      "3                           wow!!! this product is awesome... but a bit expensive??\n",
      "4                                               the laptop works perfectly fine.   \n",
      "5                                         check out the full product details here: \n",
      "6                                      great purchase!i am happy with this product.\n",
      "7                  the battry life is excelent, but the chargin cable is too short.\n",
      "8                         i can't believe it's so good! didn't expect such quality.\n",
      "9                     love this product! ???? fast delivery ??, amazing quality! ??\n",
      "10                         tbh, i wasnt expecting much, but omg, this is awesome!!\n",
      "11                            this is the best product i have ever used in my life!\n",
      "12    the shoes were comfortable, fitting nicely, and worked perfectly for jogging.\n",
      "Name: html_removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removal of HTML tags\n",
    "from bs4 import BeautifulSoup\n",
    "# extracts only the text, removing all HTML tags\n",
    "def remove_html_tags(text):\n",
    " return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "df[\"html_removed\"] = df[\"urls_removed\"].apply(remove_html_tags)\n",
    "# Display column content without truncation\n",
    "pd.set_option('display.max_colwidth', None) # Set to None for unlimited width\n",
    "print(df[\"html_removed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73789f-834e-4471-bb38-fb0707226642",
   "metadata": {},
   "source": [
    "### d. Remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2869c097-a45a-40c3-9716-8f7228e23211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the product arrived on time. packaging was great, and the quality is amazing!\n",
      "1                                          this product is just amazing! i love it.\n",
      "2       i bought this phone for $799, and it has a 120hz display. totally worth it!\n",
      "3                           wow!!! this product is awesome... but a bit expensive??\n",
      "4                                               the laptop works perfectly fine.   \n",
      "5                                         check out the full product details here: \n",
      "6                                      great purchase!i am happy with this product.\n",
      "7                  the battry life is excelent, but the chargin cable is too short.\n",
      "8                         i can't believe it's so good! didn't expect such quality.\n",
      "9                     love this product! ???? fast delivery ??, amazing quality! ??\n",
      "10                         tbh, i wasnt expecting much, but omg, this is awesome!!\n",
      "11                            this is the best product i have ever used in my life!\n",
      "12    the shoes were comfortable, fitting nicely, and worked perfectly for jogging.\n",
      "Name: emojis_removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removal of emojis (if any)\n",
    "import emoji\n",
    "# replace emoji with ''\n",
    "def remove_emojis(text):\n",
    " return emoji.replace_emoji(text, replace='')\n",
    "df[\"emojis_removed\"] = df[\"html_removed\"].apply(remove_emojis)\n",
    "# Display column content without truncation\n",
    "pd.set_option('display.max_colwidth', None) # Set to None for unlimited width\n",
    "print(df[\"emojis_removed\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7289e7-61b1-446e-8f41-9a9c5b4dd1b1",
   "metadata": {},
   "source": [
    "### e. Replace internet slang/chat words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a70c1b6a-c540-4b00-8acf-97633e93277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the product arrived on time. packaging was great, and the quality is amazing!\n",
      "1                                          this product is just amazing! i love it.\n",
      "2       i bought this phone for $799, and it has a 120hz display. totally worth it!\n",
      "3                           wow!!! this product is awesome... but a bit expensive??\n",
      "4                                               the laptop works perfectly fine.   \n",
      "5                                         check out the full product details here: \n",
      "6                                      great purchase!i am happy with this product.\n",
      "7                  the battry life is excelent, but the chargin cable is too short.\n",
      "8                         i can't believe it's so good! didn't expect such quality.\n",
      "9                     love this product! ???? fast delivery ??, amazing quality! ??\n",
      "10          to be honest, i wasnt expecting much, but oh my god, this is awesome!!\n",
      "11                            this is the best product i have ever used in my life!\n",
      "12    the shoes were comfortable, fitting nicely, and worked perfectly for jogging.\n",
      "Name: slangs_replaced, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of slang words and their replacements\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"ikr\": \"I know right\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friend forever\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "}\n",
    "\n",
    "# Function to replace slang words\n",
    "def replace_slang(text):\n",
    "    # Handle non-string / missing values safely\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "\n",
    "    # Create a list of escaped slang words\n",
    "    escaped_slang_words = []\n",
    "    for word in slang_dict.keys():\n",
    "        escaped_word = re.escape(word)  # Ensure special characters are escaped\n",
    "        escaped_slang_words.append(escaped_word)\n",
    "\n",
    "    # Join the words using '|'\n",
    "    slang_pattern = r\"\\b(\" + \"|\".join(escaped_slang_words) + r\")\\b\"\n",
    "\n",
    "    # Define a replacement function\n",
    "    def replace_match(match):\n",
    "        slang_word = match.group(0).lower()\n",
    "        return slang_dict.get(slang_word, match.group(0))\n",
    "\n",
    "    # Use regex to replace slang words with full forms\n",
    "    replaced_text = re.sub(slang_pattern, replace_match, text, flags=re.IGNORECASE)\n",
    "    return replaced_text\n",
    "\n",
    "# Apply the function to the column\n",
    "df[\"slangs_replaced\"] = df[\"emojis_removed\"].apply(replace_slang)\n",
    "\n",
    "# Display column content without truncation\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(df[\"slangs_replaced\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4cdb4-907a-4c0b-afcc-88dcb7cbfa2c",
   "metadata": {},
   "source": [
    "### f. Replace contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd2cc7f5-5278-4f46-b40d-4af47dada991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the product arrived on time. packaging was great, and the quality is amazing!\n",
      "1                                          this product is just amazing! i love it.\n",
      "2       i bought this phone for $799, and it has a 120hz display. totally worth it!\n",
      "3                           wow!!! this product is awesome... but a bit expensive??\n",
      "4                                               the laptop works perfectly fine.   \n",
      "5                                         check out the full product details here: \n",
      "6                                      great purchase!i am happy with this product.\n",
      "7                  the battry life is excelent, but the chargin cable is too short.\n",
      "8                      i cannot believe it is so good! did not expect such quality.\n",
      "9                     love this product! ???? fast delivery ??, amazing quality! ??\n",
      "10          to be honest, i wasnt expecting much, but oh my god, this is awesome!!\n",
      "11                            this is the best product i have ever used in my life!\n",
      "12    the shoes were comfortable, fitting nicely, and worked perfectly for jogging.\n",
      "Name: contractions_replaced, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Replace Contractions\n",
    "contractions_dict = {\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\",\n",
    "}\n",
    "\n",
    "# Build the regex pattern for contractions\n",
    "escaped_contractions = []\n",
    "for contraction in contractions_dict.keys():\n",
    "    # Escape special characters (e.g., apostrophes)\n",
    "    escaped_contraction = re.escape(contraction)\n",
    "    escaped_contractions.append(escaped_contraction)\n",
    "\n",
    "joined_contractions = \"|\".join(escaped_contractions)\n",
    "contractions_pattern = r\"\\b(\" + joined_contractions + r\")\\b\"\n",
    "compiled_pattern = re.compile(contractions_pattern, flags=re.IGNORECASE)\n",
    "\n",
    "# Define a function to replace contractions\n",
    "def replace_contractions(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "\n",
    "    def replace_match(match):\n",
    "        matched_word = match.group(0).lower()\n",
    "        return contractions_dict.get(matched_word, match.group(0))\n",
    "\n",
    "    return compiled_pattern.sub(replace_match, text)\n",
    "\n",
    "# Apply the function to a DataFrame column\n",
    "df[\"contractions_replaced\"] = df[\"slangs_replaced\"].apply(replace_contractions)\n",
    "\n",
    "# Display column content without truncation\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(df[\"contractions_replaced\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493c030-5c7e-4105-b40b-a323396a852b",
   "metadata": {},
   "source": [
    "### g. Remove punctuations and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df339e58-dba2-4eca-a399-6e979d8ec80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the product arrived on time packaging was great and the quality is amazing\n",
      "1                                         this product is just amazing i love it\n",
      "2        i bought this phone for 799 and it has a 120hz display totally worth it\n",
      "3                                wow this product is awesome but a bit expensive\n",
      "4                                             the laptop works perfectly fine   \n",
      "5                                       check out the full product details here \n",
      "6                                     great purchasei am happy with this product\n",
      "7                 the battry life is excelent but the chargin cable is too short\n",
      "8                     i cannot believe it is so good did not expect such quality\n",
      "9                             love this product  fast delivery  amazing quality \n",
      "10            to be honest i wasnt expecting much but oh my god this is awesome\n",
      "11                          this is the best product i have ever used in my life\n",
      "12    the shoes were comfortable fitting nicely and worked perfectly for jogging\n",
      "Name: punctuations_removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuations and special characters\n",
    "import string\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    " return text.translate(str.maketrans('', '', string.punctuation))\n",
    "# Apply the function to the column\n",
    "df[\"punctuations_removed\"] = df[\"contractions_replaced\"].apply(remove_punctuation)\n",
    "# Display column content without truncation\n",
    "pd.set_option('display.max_colwidth', None) # Set to None for unlimited width\n",
    "print(df[\"punctuations_removed\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aef7f4-2e58-4b93-88c4-af1af107272d",
   "metadata": {},
   "source": [
    "### h. Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be480e5a-7726-483c-9615-3d6a5130cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the product arrived on time packaging was great and the quality is amazing\n",
      "1                                         this product is just amazing i love it\n",
      "2              i bought this phone for  and it has a hz display totally worth it\n",
      "3                                wow this product is awesome but a bit expensive\n",
      "4                                             the laptop works perfectly fine   \n",
      "5                                       check out the full product details here \n",
      "6                                     great purchasei am happy with this product\n",
      "7                 the battry life is excelent but the chargin cable is too short\n",
      "8                     i cannot believe it is so good did not expect such quality\n",
      "9                             love this product  fast delivery  amazing quality \n",
      "10            to be honest i wasnt expecting much but oh my god this is awesome\n",
      "11                          this is the best product i have ever used in my life\n",
      "12    the shoes were comfortable fitting nicely and worked perfectly for jogging\n",
      "Name: numbers_removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    " return re.sub(r'\\d+', '', text) # Removes all numeric characters\n",
    "# Apply the function to the column\n",
    "df[\"numbers_removed\"] = df[\"punctuations_removed\"].apply(remove_numbers)\n",
    "# Display column content without truncation\n",
    "pd.set_option('display.max_colwidth', None) # Set to None for unlimited width\n",
    "print(df[\"numbers_removed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807c19b-166b-4325-8e94-96ae739814fa",
   "metadata": {},
   "source": [
    "### i. Correct spelling mistakes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c5d81da-3592-4dda-8c9c-a227b98f59ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     the product arrived on time packaging was great and the quality is amazing\n",
      "1                                         this product is just amazing i love it\n",
      "2              i bought this phone for  and it has a hz display totally worth it\n",
      "3                                wow this product is awesome but a bit expensive\n",
      "4                                             the laptop works perfectly fine   \n",
      "5                                       check out the full product details here \n",
      "6                                     great purchased am happy with this product\n",
      "7              the battery life is excellent but the charging cable is too short\n",
      "8                     i cannot believe it is so good did not expect such quality\n",
      "9                             love this product  fast delivery  amazing quality \n",
      "10            to be honest i wasnt expecting much but oh my god this is awesome\n",
      "11                          this is the best product i have ever used in my life\n",
      "12    the shoes were comfortable fitting nicely and worked perfectly for jogging\n",
      "Name: spelling_corrected, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Correct spelling mistakes\n",
    "from autocorrect import Speller\n",
    "# Initialize spell checker\n",
    "spell = Speller(lang='en')\n",
    "# Function to correct spelling\n",
    "def correct_spelling(text):\n",
    " return spell(text) # Apply correction\n",
    "# Apply the function to the column\n",
    "df[\"spelling_corrected\"] = df[\"numbers_removed\"].apply(correct_spelling)\n",
    "# Display column content without truncation\n",
    "pd.set_option('display.max_colwidth', None) # Set to None for unlimited width\n",
    "print(df[\"spelling_corrected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c5f0e-69ed-439b-b3ac-8dd7e62381b5",
   "metadata": {},
   "source": [
    "### j. Remove stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de777dce-b84e-4a33-aabd-eb651056eacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          product arrived time packaging great quality amazing\n",
      "1                                          product amazing love\n",
      "2                         bought phone hz display totally worth\n",
      "3                             wow product awesome bit expensive\n",
      "4                                   laptop works perfectly fine\n",
      "5                                    check full product details\n",
      "6                                 great purchased happy product\n",
      "7                   battery life excellent charging cable short\n",
      "8                            cannot believe good expect quality\n",
      "9                    love product fast delivery amazing quality\n",
      "10                  honest wasnt expecting much oh god awesome\n",
      "11                                  best product ever used life\n",
      "12    shoes comfortable fitting nicely worked perfectly jogging\n",
      "Name: stopwords_removed, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Define stopwords list\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = str(text)\n",
    "\n",
    "    words = text.split()  # Split text into words\n",
    "    filtered_words = []   # Store words after stopword removal\n",
    "\n",
    "    for word in words:  # Loop through each word\n",
    "        lower_word = word.lower()  # Convert to lowercase for uniform comparison\n",
    "        if lower_word not in stop_words:  # If not a stopword, keep it\n",
    "            filtered_words.append(word)\n",
    "\n",
    "    return \" \".join(filtered_words)  # Join words back into a sentence\n",
    "\n",
    "# Apply the function to the column\n",
    "df[\"stopwords_removed\"] = df[\"spelling_corrected\"].apply(remove_stopwords)\n",
    "\n",
    "# Display column content without truncation\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(df[\"stopwords_removed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a7f1b5-7673-4020-848c-d03de15c3e58",
   "metadata": {},
   "source": [
    "### k. Stemming - - reduces words to their base root by chopping off suffixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7eb6bb5-9a84-409b-8159-84320becab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     product arriv time packag great qualiti amaz\n",
      "1                                product amaz love\n",
      "2              bought phone hz display total worth\n",
      "3                    wow product awesom bit expens\n",
      "4                       laptop work perfectli fine\n",
      "5                        check full product detail\n",
      "6                      great purchas happi product\n",
      "7              batteri life excel charg cabl short\n",
      "8                cannot believ good expect qualiti\n",
      "9          love product fast deliveri amaz qualiti\n",
      "10         honest wasnt expect much oh god awesom\n",
      "11                      best product ever use life\n",
      "12        shoe comfort fit nice work perfectli jog\n",
      "Name: stemmed_words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stemming - reduces words to their base root by chopping off suffixes\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to apply stemming\n",
    "def stem_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]  # Apply stemming\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "# Apply the function\n",
    "df[\"stemmed_words\"] = df[\"stopwords_removed\"].apply(stem_text)\n",
    "\n",
    "# Display column content without truncation\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(df[\"stemmed_words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7d181-655c-4edd-abc5-a40cdcdfa6cb",
   "metadata": {},
   "source": [
    "### l. Lemmatization - reduces words to their base dictionary form (lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04d6758b-c4ed-492c-81d4-48f4025cb194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     product arrive time packaging great quality amazing\n",
      "1                                      product amaze love\n",
      "2                      buy phone hz display totally worth\n",
      "3                       wow product awesome bit expensive\n",
      "4                              laptop work perfectly fine\n",
      "5                               check full product detail\n",
      "6                            great purchase happy product\n",
      "7               battery life excellent charge cable short\n",
      "8                     can not believe good expect quality\n",
      "9              love product fast delivery amazing quality\n",
      "10               honest wasnt expect much oh god awesome\n",
      "11                             best product ever use life\n",
      "12         shoe comfortable fit nicely work perfectly jog\n",
      "Name: lemmatized, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Download required resources (safe, common names)\n",
    "nltk.download(\"wordnet\")                      # Lemmatizer dictionary\n",
    "nltk.download(\"omw-1.4\")                      # WordNet data\n",
    "nltk.download(\"averaged_perceptron_tagger\")   # POS tagger\n",
    "nltk.download(\"punkt\")                        # Tokenizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith(\"J\"):      # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith(\"V\"):    # Verb\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith(\"N\"):    # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith(\"R\"):    # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN           # Default\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "        for word, tag in pos_tags\n",
    "    ]\n",
    "\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "df[\"lemmatized\"] = df[\"stopwords_removed\"].apply(lemmatize_text)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(df[\"lemmatized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff35e1-d1ea-45c7-b84a-ff21834d0df4",
   "metadata": {},
   "source": [
    "## Step 3: Save the result to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5abebc5-0f6a-46e9-af59-f330d31c7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Processed_Reviews.csv\",encoding='latin1', index=False) # Saves without the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f37a49f4-dbfb-41a1-9ec2-0401844dbef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                          Review  \\\n",
      "0  The product arrived on time. Packaging was great, and the quality is amazing!   \n",
      "1                                       THIS PRODUCT IS JUST AMAZING! I LOVE IT.   \n",
      "2    I bought this phone for $799, and it has a 120Hz display. Totally worth it!   \n",
      "3                        Wow!!! This product is awesome... but a bit expensive??   \n",
      "4                                            The laptop works perfectly fine.      \n",
      "\n",
      "                                                     processed  \n",
      "0  [product, arrive, time, packaging, great, quality, amazing]  \n",
      "1                                       [product, amaze, love]  \n",
      "2                    [buy, phone, hz, display, totally, worth]  \n",
      "3                      [wow, product, awesome, bit, expensive]  \n",
      "4                              [laptop, work, perfectly, fine]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "nltk.download('omw-1.4')  # WordNet lexical database\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # For POS tagging\n",
    "nltk.download('punkt_tab')  # For tokenization\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller(lang='en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Dictionary of slang words and their replacements\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"ikr\": \"I know right\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friend forever\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"rofl\": \"rolling on the floor laughing\"\n",
    "}\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions_dict = {\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\"\n",
    "}\n",
    "\n",
    "# Remove any URLs that start with \"http\" or \"www\" from the text\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "# extracts only the text, removing all HTML tags\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# replace emoji with ''\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# Replace internet slang/chat words\n",
    "def replace_slang(text):\n",
    "    escaped_slang_words = []\n",
    "    for word in slang_dict.keys():\n",
    "        escaped_word = re.escape(word)\n",
    "        escaped_slang_words.append(escaped_word)\n",
    "\n",
    "    slang_pattern = r'\\b(' + '|'.join(escaped_slang_words) + r')\\b'\n",
    "\n",
    "    def replace_match(match):\n",
    "        slang_word = match.group(0)\n",
    "        return slang_dict[slang_word.lower()]\n",
    "\n",
    "    replaced_text = re.sub(slang_pattern, replace_match, text, flags=re.IGNORECASE)\n",
    "    return replaced_text\n",
    "\n",
    "# Function to expand contractions\n",
    "escaped_contractions = []\n",
    "for contraction in contractions_dict.keys():\n",
    "    escaped_contraction = re.escape(contraction)\n",
    "    escaped_contractions.append(escaped_contraction)\n",
    "\n",
    "joined_contractions = \"|\".join(escaped_contractions)\n",
    "contractions_pattern = r'\\b(' + joined_contractions + r')\\b'\n",
    "compiled_pattern = re.compile(contractions_pattern, flags=re.IGNORECASE)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace_match(match):\n",
    "        matched_word = match.group(0)\n",
    "        lower_matched_word = matched_word.lower()\n",
    "        expanded_form = contractions_dict[lower_matched_word]\n",
    "        return expanded_form\n",
    "\n",
    "    expanded_text = compiled_pattern.sub(replace_match, text)\n",
    "    return expanded_text\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Function to correct spelling using AutoCorrect\n",
    "def correct_spelling(text):\n",
    "    return spell(text)\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to lemmatize text with POS tagging\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "        for word, tag in pos_tags\n",
    "    ]\n",
    "\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Function to apply all preprocessing steps\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Step 1: Lowercasing\n",
    "    text = remove_urls(text)  # Step 2: Remove URLs\n",
    "    text = remove_html(text)  # Step 3: Remove HTML tags\n",
    "    text = remove_emojis(text)  # Step 4: Remove Emojis\n",
    "    text = replace_slang(text)  # Step 5: Replace Slang\n",
    "    text = replace_contractions(text)  # Step 6: Expand Contractions\n",
    "    text = remove_punctuation(text)  # Step 7: Remove Punctuation\n",
    "    text = remove_numbers(text)  # Step 8: Remove Numbers\n",
    "    text = correct_spelling(text)  # Step 9: Correct Spelling\n",
    "    text = remove_stopwords(text)  # Step 10: Remove Stopwords\n",
    "    text = lemmatize_text(text)  # Step 11: Lemmatization\n",
    "    text = tokenize_text(text)  # Step 12: Tokenization\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Review.csv\", encoding=\"latin1\")  # Replace with your file\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "df[\"processed\"] = df[\"Review\"].apply(preprocess_text)\n",
    "\n",
    "df.to_csv(\"Processed_Reviews2.csv\", index=False)\n",
    "# Display the first few rows\n",
    "print(df[[\"Review\", \"processed\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b69db-c0b5-4ebe-8c1b-138f0cb3f64d",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dcb8336-3c05-4301-9a7b-cd790047fb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\IM11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded with encoding: utf-8\n",
      "\n",
      "=== Issues Found in Review Column ===\n",
      "- Total rows: 52\n",
      "- Missing/empty (after str): 0\n",
      "- Has URL: 0\n",
      "- Has HTML tags: 0\n",
      "- Has emoji: 1\n",
      "- Has numbers: 4\n",
      "- Has punctuation: 42\n",
      "- Has non-ASCII chars (smart quotes etc.): 8\n",
      "- Has multiple spaces: 4\n",
      "\n",
      "Emoji examples:\n",
      "1. It’s a great place to study but for me the are lot of things that need to improve?? wifi connection is the priority since there a lot of bad connections, the things that i have the issue since first s...\n",
      "\n",
      "Non-ASCII examples:\n",
      "1. I’m having a pretty good time here, happy to meet all of the W people.\n",
      "2. UNITEN is a solid choice for students interested in engineering, IT, or energy-related fields, offering strong industry connections and modern facilities. However, its specialized focus and location m...\n",
      "3. As a UNITEN student, I’ve had a great experience so far. Lecturers are knowledgeable and approachable, always willing to help when needed. The campus facilities are well-maintained, including the labs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IM11\\AppData\\Local\\Temp\\ipykernel_4000\\3991352919.py:170: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: UNITENReview_processed.csv\n",
      "Preview:\n",
      "                                                                                                                                                                                                                                                                                                                                                         Review  \\\n",
      "0                                                                                                                                                                                                                                                                                                          Im happy with uniten actually, even the people are W   \n",
      "1                                                                                                                                                                                                                                                                                        I’m having a pretty good time here, happy to meet all of the W people.   \n",
      "2                                                                                                                                                                                                                                                                                                                   a very neutral place in terms of everything   \n",
      "3                                                                                                                                                                                                   I would say Uniten it's  a good university  but there is some issue need to be improved such as transportation,wifi networks and other facilities  as well.   \n",
      "4   UNITEN is well-regarded, particularly for its strong engineering, computer science, and business programs. It has a solid reputation in Malaysia, especially in energy-related fields. The negative part is the facilities such as the swimming pool are close since my first year till 3 year now and there are limited parking so it's make hard to find.   \n",
      "\n",
      "                                                                                                                                                                                                                               processed_text  \n",
      "0                                                                                                                                                                                                       im happy unite actually even people w  \n",
      "1                                                                                                                                                                                                  i ’ m pretty good time happy meet w people  \n",
      "2                                                                                                                                                                                                               neutral place term everything  \n",
      "3                                                                                                                                                would say united good university issue need improve transportationwifi network facility well  \n",
      "4  united wellregarded particularly strong engineering computer science business program solid reputation malaysia especially energyrelated field negative part facility swim pool close since first year till year limit park make hard find  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "# ----------------------------\n",
    "# 0) NLTK setup\n",
    "# ----------------------------\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "spell = Speller(lang=\"en\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load dataset (robust encoding)\n",
    "# ----------------------------\n",
    "file_path = \"UNITENReview.csv\"   # <- your file name\n",
    "encodings_to_try = [\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"]\n",
    "\n",
    "df = None\n",
    "used_encoding = None\n",
    "for enc in encodings_to_try:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=enc)\n",
    "        used_encoding = enc\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "\n",
    "if df is None:\n",
    "    # last resort: ignore bad characters\n",
    "    df = pd.read_csv(file_path, encoding=\"cp1252\", encoding_errors=\"ignore\")\n",
    "    used_encoding = \"cp1252 (ignored errors)\"\n",
    "\n",
    "print(\"Loaded with encoding:\", used_encoding)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Identify issues in \"Review\" column\n",
    "# ----------------------------\n",
    "col = \"Review\"\n",
    "if col not in df.columns:\n",
    "    raise ValueError(f'Column \"{col}\" not found. Available columns: {list(df.columns)}')\n",
    "\n",
    "review_series = df[col].astype(str)\n",
    "\n",
    "def has_url(s): return bool(re.search(r\"http\\S+|www\\S+\", s))\n",
    "def has_html(s): return bool(re.search(r\"<[^>]+>\", s))\n",
    "def has_emoji(s): return any(ch in emoji.EMOJI_DATA for ch in s)\n",
    "def has_numbers(s): return bool(re.search(r\"\\d+\", s))\n",
    "def has_punct(s): return any(ch in string.punctuation for ch in s)\n",
    "def has_non_ascii(s): return any(ord(ch) > 127 for ch in s)\n",
    "def has_extra_spaces(s): return bool(re.search(r\"\\s{2,}\", s))\n",
    "\n",
    "stats = {\n",
    "    \"Total rows\": len(review_series),\n",
    "    \"Missing/empty (after str)\": int((review_series.str.strip() == \"\").sum()),\n",
    "    \"Has URL\": int(review_series.apply(has_url).sum()),\n",
    "    \"Has HTML tags\": int(review_series.apply(has_html).sum()),\n",
    "    \"Has emoji\": int(review_series.apply(has_emoji).sum()),\n",
    "    \"Has numbers\": int(review_series.apply(has_numbers).sum()),\n",
    "    \"Has punctuation\": int(review_series.apply(has_punct).sum()),\n",
    "    \"Has non-ASCII chars (smart quotes etc.)\": int(review_series.apply(has_non_ascii).sum()),\n",
    "    \"Has multiple spaces\": int(review_series.apply(has_extra_spaces).sum()),\n",
    "}\n",
    "\n",
    "print(\"\\n=== Issues Found in Review Column ===\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "\n",
    "# Show a few examples for each issue (so you can prove you identified them)\n",
    "def show_examples(title, mask_func, n=3):\n",
    "    examples = review_series[review_series.apply(mask_func)].head(n).tolist()\n",
    "    if examples:\n",
    "        print(f\"\\n{title} examples:\")\n",
    "        for i, ex in enumerate(examples, 1):\n",
    "            print(f\"{i}. {ex[:200]}{'...' if len(ex) > 200 else ''}\")\n",
    "\n",
    "show_examples(\"URL\", has_url)\n",
    "show_examples(\"HTML\", has_html)\n",
    "show_examples(\"Emoji\", has_emoji)\n",
    "show_examples(\"Non-ASCII\", has_non_ascii)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Pre-processing pipeline (based on typical issues)\n",
    "# ----------------------------\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"ikr\": \"i know right\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"bff\": \"best friend forever\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "}\n",
    "\n",
    "contractions_dict = {\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\",\n",
    "}\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "def replace_slang(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    escaped = [re.escape(w) for w in slang_dict.keys()]\n",
    "    pattern = r\"\\b(\" + \"|\".join(escaped) + r\")\\b\"\n",
    "\n",
    "    def repl(m):\n",
    "        w = m.group(0).lower()\n",
    "        return slang_dict.get(w, w)\n",
    "\n",
    "    return re.sub(pattern, repl, text, flags=re.IGNORECASE)\n",
    "\n",
    "# contraction regex compiled once\n",
    "escaped_contractions = [re.escape(c) for c in contractions_dict.keys()]\n",
    "contractions_pattern = r\"\\b(\" + \"|\".join(escaped_contractions) + r\")\\b\"\n",
    "compiled_contractions = re.compile(contractions_pattern, flags=re.IGNORECASE)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    def repl(m):\n",
    "        w = m.group(0).lower()\n",
    "        return contractions_dict.get(w, w)\n",
    "\n",
    "    return compiled_contractions.sub(repl, text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    # WARNING: slow on big datasets. Keep it because your exercise includes it.\n",
    "    return spell(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w.lower() not in stop_words]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    words = word_tokenize(text)\n",
    "    tags = pos_tag(words)\n",
    "    lemmas = [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in tags]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    text = text.lower()\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = replace_slang(text)\n",
    "    text = replace_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = correct_spelling(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"processed\"] = df[col].apply(preprocess_text)\n",
    "\n",
    "# Optional: store as string instead of list (CSV-friendly)\n",
    "df[\"processed_text\"] = df[\"processed\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Save result\n",
    "# ----------------------------\n",
    "output_path = \"UNITENReview_processed.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nSaved:\", output_path)\n",
    "print(\"Preview:\")\n",
    "print(df[[col, \"processed_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59176b4d-0886-4ff4-9c8f-12a066da300f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
